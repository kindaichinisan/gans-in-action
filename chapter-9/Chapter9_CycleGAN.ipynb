{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9wyQVXTmwiz"
      },
      "source": [
        "# Chapter 9: CycleGAN\n",
        "This Colab / Ipython notebook will walk you through: \n",
        "1. Install extra dependencies that Colab does not natively support\n",
        "2. Download the apple2orange dataset using the `%%bash` magic.\n",
        "3. Create the `DataLoader()` class that server as our core data structure\n",
        "4. Do all the necessary imports\n",
        "5. Define the CycleGAN in the following steps:\n",
        "\n",
        "> 5.1 Define the key hyper-parameters and flow\n",
        "\n",
        "> 5.2 Define the static methods we are going to be reusing in the generator and discriminator\n",
        "\n",
        "> 5.3 Build the generator and the discriminator\n",
        "\n",
        "> 5.4 Define the helper function for sampling our translated images\n",
        "\n",
        "> 5.5 Define the explicit training loop\n",
        "\n",
        "6. Fit the CycleGAN and talk about the results!\n",
        "\n",
        "\n",
        "--- \n",
        "\n",
        "## Helper functions \n",
        "1. Pip installing directly from GitHub the `keras-contrib` folder, which we will need for the `InstanceNormalization`. We tested this git-hash version of the repository: \n",
        "`46fcdb9384b3bc9399c651b2b43640aa54098e64`\n",
        "2. Downloading the dataset to our cloud-based—ephemeral—storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "aS7MLTe97Tz7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "yKbFxXyASvkb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datasets/apple2orange\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "import os\n",
        "# import requests\n",
        "\n",
        "FILE='apple2orange'\n",
        "#URL=https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/$FILE.zip #not valid anymore\n",
        "# URL=r'https://www.kaggle.com/datasets/balraj98/apple2orange-dataset?resource=download'\n",
        "# ZIP_FILE=rf'datasets/{FILE}.zip'\n",
        "TARGET_DIR=rf'datasets/{FILE}'\n",
        "print(f'{TARGET_DIR}')\n",
        "#wget -N URL -O $ZIP_FILE\n",
        "# response = requests.get(URL)\n",
        "\n",
        "# # Open a file in binary write mode and write the content\n",
        "# with open(ZIP_FILE, 'wb') as file:\n",
        "#     file.write(response.content)\n",
        "\n",
        "os.makedirs(TARGET_DIR, exist_ok=True)\n",
        "\n",
        "#download from https://www.kaggle.com/datasets/balraj98/apple2orange-dataset?resource=download and manually extract and put into TARGET_DIR\n",
        "#unzip $ZIP_FILE -d ./datasets/\n",
        "#rm $ZIP_FILE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LX79_4iEpIeu"
      },
      "source": [
        "## DataLoader\n",
        "We define our key data-holding object in the (hidden) cell below. With it, we can:\n",
        "1. `load_data` from disk based on the dataset name, which we will define in `CycleGAN`'s `__init__`\n",
        "2. `load_batch` during training—note this defined as a generator for greater efficiency\n",
        "3. A helper functions called `imread`, which we use in `load_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "Ic8zbMaV7ldk"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import scipy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                #img = scipy.misc.imresize(img, self.img_res)\n",
        "                img = cv2.resize(img, self.img_res, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                #img = scipy.misc.imresize(img, self.img_res)\n",
        "                img = cv2.resize(img, self.img_res, interpolation=cv2.INTER_CUBIC)\n",
        "            imgs.append(img)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path_A = glob('datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
        "        path_B = glob('datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
        "        #path_A = glob(os.path.join('datasets', self.dataset_name, f'{data_type}A', '*'))\n",
        "        #path_B = glob(os.path.join('datasets', self.dataset_name, f'{data_type}B', '*'))\n",
        "\n",
        "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        # Sample n_batches * batch_size from each path list so that model sees all\n",
        "        # samples from both domains\n",
        "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_A, img_B in zip(batch_A, batch_B):\n",
        "                #print(f'img_A: {img_A}')\n",
        "                #print(f'img_B: {img_B}')\n",
        "                img_A = self.imread(img_A)\n",
        "                img_B = self.imread(img_B)\n",
        "\n",
        "                #img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "                img_A = cv2.resize(img_A, self.img_res, interpolation=cv2.INTER_CUBIC)\n",
        "                #img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "                img_B = cv2.resize(img_B, self.img_res, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def imread(self, path):\n",
        "        #return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
        "        # Read the image using OpenCV (default is BGR format)\n",
        "        image_bgr = cv2.imread(path)\n",
        "\n",
        "        # Check if the image was loaded successfully\n",
        "        if image_bgr is None:\n",
        "            raise ValueError(\"Error: Image not found or unable to load.\")\n",
        "\n",
        "        # Convert the BGR image to RGB\n",
        "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert the image to float\n",
        "        image_float = image_rgb.astype(np.float32)  # Use np.float32 for precision\n",
        "\n",
        "        return image_float\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1enRl0furFT1"
      },
      "source": [
        "## Good old imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "4QyCA6Lq7aMc",
        "outputId": "15a472d0-43c4-4eac-f8a0-7b0e4fc6e937"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "#import scipy\n",
        "from tensorflow.keras.datasets import mnist\n",
        "#from keras_contrib.layers.normalization import InstanceNormalization\n",
        "from tensorflow.keras.layers import GroupNormalization\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "#from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "#from tensorflow.keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lo-E3tlCrH38"
      },
      "source": [
        "## CycleGAN itself\n",
        "\n",
        "1. We define some parameters\n",
        "2. We build the discriminators and compile them\n",
        "3. We build the generators and compile them (these are a bit more complicated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eDZ8ZPpc7h09"
      },
      "outputs": [],
      "source": [
        "class CycleGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 128\n",
        "        self.img_cols = 128\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        # Configure data loader\n",
        "        self.dataset_name = 'apple2orange'\n",
        "        # Use the DataLoader object to import a preprocessed dataset\n",
        "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
        "                                      img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2**4)\n",
        "        self.disc_patch = (patch, patch, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 32\n",
        "        self.df = 64\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
        "        self.lambda_id = 0.9 * self.lambda_cycle    # Identity loss\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        \n",
        "        # Build and compile the discriminators\n",
        "        self.d_A = self.build_discriminator()\n",
        "        self.d_B = self.build_discriminator()\n",
        "        self.d_A.compile(loss='mse',\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=['accuracy'])\n",
        "        self.d_B.compile(loss='mse',\n",
        "                         optimizer=optimizer,\n",
        "                         metrics=['accuracy'])\n",
        "\n",
        "        #-------------------------\n",
        "        # Construct Computational\n",
        "        #   Graph of Generators\n",
        "        #-------------------------\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator()\n",
        "        self.g_BA = self.build_generator()\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "\n",
        "        # For the combined model we will only train the generators\n",
        "        self.d_A.trainable = False\n",
        "        self.d_B.trainable = False\n",
        "\n",
        "        # Discriminators determines validity of translated images\n",
        "        valid_A = self.d_A(fake_A)\n",
        "        valid_B = self.d_B(fake_B)\n",
        "\n",
        "        # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[valid_A, valid_B,\n",
        "                                       reconstr_A, reconstr_B,\n",
        "                                       img_A_id, img_B_id])\n",
        "        self.combined.compile(loss=['mse', 'mse',\n",
        "                                    'mae', 'mae',\n",
        "                                    'mae', 'mae'],\n",
        "                              loss_weights=[1, 1,\n",
        "                                            self.lambda_cycle, self.lambda_cycle,\n",
        "                                            self.lambda_id, self.lambda_id],\n",
        "                              optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YyhyRqWrdy5"
      },
      "source": [
        "## Static methods\n",
        "Two things to pay attention to:\n",
        "1. We are using a `CycleGAN` class that inherits from the `CycleGAN` class from the previous cell. As far as I know, this is the best way of defining a class across multiple cells for educational purposes. But I agree with Joel Grus—this is not the best way, but I still like notebooks.\n",
        "2. We are using `@staticmethod`, which allows us to shrink the code size by further ~25 lines. For more on [static methods I found this SO answer to be quite good](https://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod). We will use these later to more compactly write both the generator and the discriminator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Dws0Wp-EU_2l"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      @staticmethod\n",
        "      def conv2d(layer_input, filters, f_size=4, normalization=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size,\n",
        "                   strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if normalization:\n",
        "            #d = InstanceNormalization()(d)\n",
        "            d = GroupNormalization(groups=-1)(d)\n",
        "        return d\n",
        "      \n",
        "      @staticmethod\n",
        "      def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            \"\"\"Layers used during upsampling\"\"\"\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1,\n",
        "                       padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            #u = InstanceNormalization()(u)\n",
        "            u = GroupNormalization(groups=-1)(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bSnp4kqysP95"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W-RptWJpR8SR"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_generator(self):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "        # Image input\n",
        "        d0 = Input(shape=self.img_shape)\n",
        "\n",
        "        # Downsampling\n",
        "        d1 = self.conv2d(d0, self.gf)\n",
        "        d2 = self.conv2d(d1, self.gf * 2)\n",
        "        d3 = self.conv2d(d2, self.gf * 4)\n",
        "        d4 = self.conv2d(d3, self.gf * 8)\n",
        "\n",
        "        # Upsampling\n",
        "        u1 = self.deconv2d(d4, d3, self.gf * 4)\n",
        "        u2 = self.deconv2d(u1, d2, self.gf * 2)\n",
        "        u3 = self.deconv2d(u2, d1, self.gf)\n",
        "\n",
        "        u4 = UpSampling2D(size=2)(u3)\n",
        "        output_img = Conv2D(self.channels, kernel_size=4,\n",
        "                            strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "        return Model(d0, output_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hl6cWO0tsSKL"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5WDZgbZISCNa"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "    def build_discriminator(self):\n",
        "      img = Input(shape=self.img_shape)\n",
        "\n",
        "      d1 = self.conv2d(img, self.df, normalization=False)\n",
        "      d2 = self.conv2d(d1, self.df * 2)\n",
        "      d3 = self.conv2d(d2, self.df * 4)\n",
        "      d4 = self.conv2d(d3, self.df * 8)\n",
        "\n",
        "      validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "      return Model(img, validity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GAL7AoG-sT2e"
      },
      "source": [
        "## Sampling function\n",
        "This is how we got the output in the Chapter. We need to run in special mode to get the translated images out, because now we are in \"testing\"/evaluation mode.\n",
        "\n",
        "We then plot the whole cycle:\n",
        "\n",
        "$ A -> \\hat B -> \\hat A$\n",
        "\n",
        "As well as:\n",
        "\n",
        "$ B -> \\hat A -> \\hat B$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ssU_MY9NTDdF"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      def sample_images(self, epoch, batch_i):\n",
        "        r, c = 2, 3\n",
        "\n",
        "        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
        "        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
        "        \n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        imgfolder=os.path.join(\"images\",self.dataset_name)\n",
        "        os.makedirs(imgfolder, exist_ok=True)\n",
        "        imgfilename=f'{epoch}_{batch_i}.png'\n",
        "        imgfilepath=os.path.join(imgfolder, imgfilename)\n",
        "        print(imgfilepath)\n",
        "        fig.savefig(imgfilepath)\n",
        "        #plt.show()\n",
        "\n",
        "        # Close the figure to free memory\n",
        "        plt.close(fig)\n",
        "        \n",
        "        # Optionally, you can delete large variables if they are not needed further\n",
        "        del imgs_A, imgs_B, fake_A, fake_B, reconstr_A, reconstr_B, gen_imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "otZDj-Rstu2q"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Here is where the real magic happens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iOrSHLyTSTYt"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(CycleGAN):\n",
        "      def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
        "\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                print(f'epoch: {epoch}, batch_i: {batch_i}')\n",
        "                print(f'imgs_A.shape: {imgs_A.shape}')\n",
        "                print(f'imgs_B.shape: {imgs_B.shape}')\n",
        "                print(f'valid.shape: {valid.shape}')\n",
        "                print(f'fake.shape: {fake.shape}')\n",
        "\n",
        "                # Translate images to opposite domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
        "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                # Total discriminator loss\n",
        "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "                # ------------------\n",
        "                #  Train Generators\n",
        "                # ------------------\n",
        "\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                                      [valid, valid,\n",
        "                                                       imgs_A, imgs_B,\n",
        "                                                       imgs_A, imgs_B])\n",
        "                # If at save interval => plot the generated image samples\n",
        "                if batch_i % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o1KuPTW7uj-K"
      },
      "source": [
        "## Train the CycleGAN\n",
        "\n",
        "Because we used the object-oriented design, we now need to instantiate the CycleGAN and run the `train`ing method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1975
        },
        "colab_type": "code",
        "id": "045EjDTmSUBl",
        "outputId": "4e65cf4b-6404-4525-fff0-3c41d431526f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hwenjun\\.conda\\envs\\gan_in_actions\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hwenjun\\.conda\\envs\\gan_in_actions\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000236F0EAEF20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 915ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.00986135..0.7684972].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.009819269..1.017268].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\0_0.png\n",
            "epoch: 0, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 511ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492ms/step\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000236CE959BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "epoch: 0, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455ms/step\n",
            "epoch: 0, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441ms/step\n",
            "epoch: 0, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "epoch: 0, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 485ms/step\n",
            "epoch: 1, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0334329..1.0229129].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0377298..1.0687921].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\1_0.png\n",
            "epoch: 1, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466ms/step\n",
            "epoch: 1, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "epoch: 1, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478ms/step\n",
            "epoch: 1, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465ms/step\n",
            "epoch: 1, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453ms/step\n",
            "epoch: 2, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.116276026..1.0856273].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.006575525..1.0034353].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\2_0.png\n",
            "epoch: 2, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464ms/step\n",
            "epoch: 2, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "epoch: 2, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458ms/step\n",
            "epoch: 2, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457ms/step\n",
            "epoch: 2, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 496ms/step\n",
            "epoch: 3, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0025811791..1.003822].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0043466687..1.0394646].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\3_0.png\n",
            "epoch: 3, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 481ms/step\n",
            "epoch: 3, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600ms/step\n",
            "epoch: 3, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425ms/step\n",
            "epoch: 3, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439ms/step\n",
            "epoch: 3, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 413ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n",
            "epoch: 4, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.057257175..1.0733073].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.010374546..1.0146024].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\4_0.png\n",
            "epoch: 4, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468ms/step\n",
            "epoch: 4, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486ms/step\n",
            "epoch: 4, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428ms/step\n",
            "epoch: 4, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 509ms/step\n",
            "epoch: 4, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447ms/step\n",
            "epoch: 5, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0051585436..0.98920417].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.048226893..1.068865].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\5_0.png\n",
            "epoch: 5, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452ms/step\n",
            "epoch: 5, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 418ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408ms/step\n",
            "epoch: 5, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461ms/step\n",
            "epoch: 5, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 460ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459ms/step\n",
            "epoch: 5, batch_i: 5\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483ms/step\n",
            "epoch: 6, batch_i: 0\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.02829349..1.0089959].\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.03159085..1.0330423].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images\\apple2orange\\6_0.png\n",
            "epoch: 6, batch_i: 1\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 486ms/step\n",
            "epoch: 6, batch_i: 2\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475ms/step\n",
            "epoch: 6, batch_i: 3\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472ms/step\n",
            "epoch: 6, batch_i: 4\n",
            "imgs_A.shape: (64, 128, 128, 3)\n",
            "imgs_B.shape: (64, 128, 128, 3)\n",
            "valid.shape: (64, 8, 8, 1)\n",
            "fake.shape: (64, 8, 8, 1)\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458ms/step\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "cycle_gan = CycleGAN()\n",
        "cycle_gan.train(epochs=100, batch_size=64, sample_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YiIjwulwu1sZ"
      },
      "source": [
        "## Now we can look at the results!\n",
        "\n",
        "You may have three observations:\n",
        "1. The progress can be slow—even on the Colab GPU or on CPU.\n",
        "2. The initial images can be quite bad and make no sense at all, but worry not—soon enough, the images will manage at least a decent reconstruction.\n",
        "3. The later reconstructions are generally mixed: some look very convincing—such as the ones below—but some are merely an orange \"repainting\" of the apples and vice versa. Even the original authors of the paper mention this problem: CycleGAN is unable to substantially change the structure of the image, but only \"restylize\" it. So beware of this limitation.\n",
        "\n",
        "![Good examples](https://i.ibb.co/n00BW1F/163-10.png)\n",
        "\n",
        "Also, this is a good introduction to the reality of being a GAN researcher / practitioner. Most flashy results still tend to be cherry picked and most papers—e.g. [BigGAN being a notable exception](https://arxiv.org/abs/1809.11096)—do not share failure cases. BigGAN—the state of the art as of today (20/11/2018)—even [publishes Google folders worth of examples](https://drive.google.com/drive/folders/1SRYj8Ou1JZ4e09LqeawDHcUvoeY78tPO). "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chapter9_CycleGAN.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
